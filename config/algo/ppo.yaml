# defaults:
#- policy: ppo
#- optim: base

name: "ppo"

gamma: 0.99
gae_lambda: 0.95
ent_coef: 0.01
vf_coef: 0.5

batch_size: 32
n_steps: 2048
